\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{amsmath}

% Make section headings a bit smaller & tighter
\titleformat{\section}{\normalsize\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{1ex plus 0.2ex minus 0.2ex}{0.6ex}

\begin{document}

\begin{center}
    {\LARGE\bfseries ESE 3060 Final Project - Part 1}\\[0.3ex]
    {\normalsize Ani Petrosyan, Lakshman Swaminathan}\\[0.2ex]
    {\small December 7, 2025}
\end{center}

\section{Hypothesis}

Our goal is to reduce the wall-clock training time of \texttt{airbench94.py} on CIFAR-10 while staying as close as possible to the original 94.0\% test accuracy.
Our hypothesis is that (1) precomputing deterministic image transforms, (2) adding residual connections to the convolutional blocks, (3) tuning the batch size, learning rate, and number of epochs, and (4) optimizing data movement to the GPU will shorten training time by around 15--20\% while keeping test accuracy within about 0.5--1.0 percentage points of the 94.0\% baseline.

\section{Methodology}

To test this hypothesis, we started from the provided \texttt{airbench94.py} baseline and made a small set of targeted changes aimed at either (a) removing redundant computation or (b) improving convergence per unit time:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Static precomputation of image transforms.}
    Normalization, flipping, and padding are computed once in \texttt{CifarLoader.\_\_init\_\_} and stored in \texttt{self.proc\_images}, instead of being recomputed every epoch.
    During training, the loader only does the remaining random crop on precomputed padded images.

    \item \textbf{Residual connections in \texttt{ConvGroup}.}
    Each \texttt{ConvGroup} now has a residual path: the pooled input is passed through a 1$\times$1 convolution ``downsample'' layer and added back to the main convolutional path before the GELU activation.
    This is meant to improve gradient flow and let the network learn near-identity mappings more easily, so it can converge in fewer effective epochs.


    \item \textbf{Hyperparameter changes.}
    We increased the batch size from 1024 to 1536 and scaled the learning rate from 11.5 to 17.25 (1.5$\times$, matching the batch-size change).
    In the write-up I describe reducing the number of epochs (e.g., from 9.9 to 8.0) to trade a small loss in optimization for faster overall runtime.
    These changes are intended to better use GPU memory and reduce per-epoch overhead while keeping training dynamics similar.

    \item \textbf{Optimized data loading and device transfers.}
    Images are converted to the final dtype/layout and moved to the GPU once, with non-blocking transfers and labels cast to \texttt{long} on the correct device.
    This removes per-batch conversion and host-to-device overhead that does not contribute to learning.
\end{itemize}

To evaluate these changes, we ran the baseline and the modified script on the same GPU and recorded (a) final CIFAR-10 test accuracy and (b) wall-clock runtime.
The detailed experiment log, including commands and multiple runs, is provided in the separate appendix.

\section{Results \& Conclusions}

In 5 runs, the baseline achieved an average of 94.01\% test accuracy with a runtime of roughly 1m 54.381s (real category in the logs).
Over 5 runs, the modified version (``modified'' variant) ran in an average of 1m 48.074s, which is roughly an 5.3\% reduction in wall-clock time, but its accuracy dropped slightly to around 92.74\% (a loss of about 1.3 percentage points compared to the baseline).

These results partially support the hypothesis: the combination of precomputed transforms, residual connections, and hyperparameter / data-loading tweaks clearly reduces training time, and the accuracy remains very close to the original 94.0\% target.
However, the small accuracy drop suggests there is still a trade-off between speed and performance.
In future iterations, I would tune the number of epochs and learning rate more finely (or add mild regularization changes) to see if I can recover the lost $\sim$1.3 percentage points while keeping most of the $\sim$5.3\% speed-up.






\clearpage
\appendix
\section{Appendix}

\subsection{Change 1: Static Precomputation of Image Transforms}

\textbf{Hypothesis:} The original implementation recomputes normalization, flipping, and padding
operations every epoch during training. Since normalization and flipping are deterministic
transformations that do not depend on random seeds per batch, we can precompute these once during
initialization and reuse them across all epochs, eliminating redundant computation.

\textbf{What Changed:} Modified the \texttt{CifarLoader.\_\_init\_\_} method to precompute normalized
images, flipped images, and padded images once during initialization (lines 135--150 in
\texttt{airbench94-modified.py}). The \texttt{\_\_iter\_\_} method now selects from these precomputed
tensors instead of recomputing them each epoch.

\textbf{Solution:} All static transforms (normalization, flipping, padding) are computed once in the
\texttt{\_\_init\_\_} method within a \texttt{torch.no\_grad()} context. 

The normalized images are
stored in \texttt{self.proc\_images['norm']}, 

flipped images in \texttt{self.proc\_images['flip']},
and padded images in \texttt{self.proc\_images['pad']}. During iteration, we only perform the
dynamic random crop operation on the precomputed padded tensor, and apply epoch-based flipping by
selecting from precomputed tensors. This eliminates redundant normalization and padding operations
that were previously executed every epoch.

\subsection{Change 2: Residual Connections in ConvGroup}

\textbf{Hypothesis:} Adding residual (skip) connections to the \texttt{ConvGroup} blocks will
enable deeper gradient flow during backpropagation, potentially allowing the network to learn
identity mappings more easily and converge faster. This architectural pattern has been shown to
improve training dynamics in deep networks.


\textbf{What Changed:} Added a residual connection pathway to the \texttt{ConvGroup} class (in our
modified \texttt{airbench94-modified.py}). The residual path consists of a 1$\times$1 convolution to
match channel dimensions, applied after pooling the identity tensor. The final output is the sum of
the main path and the residual path, followed by activation.


\textbf{Solution:} Each \texttt{ConvGroup} now includes a \texttt{downsample} module that projects
the input to match the output channel dimensions. In the forward pass, the identity is preserved,
pooled, and passed through the \texttt{downsample} module. The main convolutional path output is
then added to this processed identity before the final activation. This creates a residual learning
pathway:
\[
\text{output} = \mathrm{GELU}\left(\text{conv\_path}(x) + \text{downsample}(\text{pool}(x))\right)
\]

\textbf{Hypothesis:} With the improved architecture (residual connections) and more efficient data loading
(static precomputation), it is possible to achieve equal or improved accuracy in fewer epochs.
Additionally, increasing batch size can improve GPU utilization and training speed, while
maintaining training dynamics by scaling the learning rate proportionally.

\textbf{What Changed:} Reduced \texttt{train\_epochs} from 9.9 to 8.0, increased
\texttt{batch\_size} from 1024 to 1536 (50\% increase), and scaled \texttt{lr} from 11.5 to 17.25
(proportional to batch size increase: $11.5 \times 1.5 = 17.25$).

\textbf{Solution:} The reduced epoch count leverages faster convergence enabled by residual
connections. The larger batch size improves GPU memory utilization and reduces the number of
gradient updates per epoch. The learning rate is scaled linearly with batch size to maintain
equivalent gradient step magnitudes, following the principle that effective learning rate should
scale with batch size when using the same number of epochs.

\subsection{Change 4: Optimized Data Loading Pipeline}

\textbf{Hypothesis:} Moving data to GPU earlier and using non-blocking transfers can overlap data
transfer with computation, reducing overall training time. Additionally, ensuring labels are
properly typed and on the correct device avoids runtime type conversions.

\textbf{What Changed:} Modified data loading to use explicit device handling (line 100), convert
images to GPU-friendly format immediately (lines 115--121), and ensure labels are properly typed as
long integers on the correct device (line 111). Used \texttt{non\_blocking=True} for asynchronous
transfers.

\textbf{Solution:} Images are converted to half precision, normalized, permuted to channels-last
format, and moved to GPU in a single pipeline during initialization. Labels are explicitly cast to
\texttt{long} and moved to the device. This ensures all data is in the optimal format before
training begins, eliminating per-batch conversion overhead.


\end{document}
